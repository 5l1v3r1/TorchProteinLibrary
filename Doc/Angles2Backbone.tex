This layer takes a tensor of dihedral angles $\phi_i, \psi_i$ (batch size, 2, number of amino-acids) 
and outputs coordinates of N, CA, C atoms $\mathbf{x}_i$ (number of atoms * 3):

\begin{lstlisting}
from ProteinClassesLibrary import Angles2Backbone

input_angles = torch.FloatTensor(batch_size, 2, num_amino_acids)
num_aa = torch.IntTensor(batch_size).fill_(num_amino_acids)
a2b = Angles2Backbone()
coords = a2b(Variable(input_angles.cuda()), Variable(num_aa.cuda()))
\end{lstlisting}


\subsection{Forward pass}

The position of the $i$-th atom in the chain is obtained by:
$$x_i = B_0 \cdot \dots \cdot B_i x_0$$
where $B_i$ is the transformation matrix, parametrized by dihedral angles.
During the forward pass, we save the products of transformation matrixes for each atom:
$$A_i = B_0 \dots \cdot B_i$$

\subsection{Backward pass}
Notice, that the atom $3j$, where $j$ is integer is always N, $3j + 1$ is CA and $3j+2$ is C. Thus
transformation matrixes $B_{3j+1}$ and $B_{3j+2}$ depend on the angles $\phi_j$ and $\psi_j$ correspondingly.
During the backward pass, we first compute gradients of each atomic coordinate w.r.t each angle:
$$
\frac{\partial \mathbf{x}_i}{\partial \phi_j} = B_0 \dots \frac{\partial B_{3j+1}}{\partial \phi_j} \dots B_i \cdot \mathbf{x}_0
$$

$$
\frac{\partial \mathbf{x}_i}{\partial \psi_j} = B_0 \dots \frac{\partial B_{3j+2}}{\partial \psi_j} \dots B_i \cdot \mathbf{x}_0
$$

We can rewrite these expressions using the matrixes $A$, that we saved during the forward pass:
$$
\frac{\partial \mathbf{x}_i}{\partial \phi_j} = A_{3j}  \frac{\partial B_{3j+1}}{\partial \phi_j} \cdot A{3j+1}^{-1} A_{i} \cdot \mathbf{x}_0
$$

$$
\frac{\partial \mathbf{x}_i}{\partial \psi_j} = A_{3j+1}  \frac{\partial B_{3j+2}}{\partial \psi_j} \cdot A{3j+2}^{-1} A_{i} \cdot \mathbf{x}_0
$$

We also notice that transformation matrixes have two properties:
\begin{enumerate}
    \item Product of transformation matrixes is a transformation matrix, therefore matrixes $A$ are transformation matrixes
    \item Inverse of transformation matrix can be computed as follows:

    If $
    T = \begin{bmatrix}
        R & \mathbf{x} \\
        0 & 1
    \end{bmatrix} 
    $
    then $T^{-1} = \begin{bmatrix}
        R^{T} & -R^{T}\cdot\mathbf{x} \\
        0 & 1
    \end{bmatrix} 
    $
\end{enumerate}
Therefore we can compute all the derivatives simultaneously on GPU. To compute the derivatives of input of the layer with respect to the output we just have to 
calculate the following sums:
$$\sum_i \frac{\partial L_k}{\mathbf{x}_i}\frac{\partial \mathbf{x}_i}{\partial \phi_j}$$ and 
$$\sum_i \frac{\partial L_k}{\mathbf{x}_i}\frac{\partial \mathbf{x}_i}{\partial \psi_j}$$